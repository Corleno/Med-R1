{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60109667",
   "metadata": {},
   "source": [
    "Data is saved in /data/datasets/OmniMedVQA/OmniMedVQA \n",
    "\n",
    "Checkpoint is saved in /home/fayang/checkpoints/Qwen2.5-VL-3B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71079a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25991839",
   "metadata": {},
   "source": [
    "# Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f125d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 159 items from /data/datasets/OmniMedVQA/OmniMedVQA/QA_information/Open-access/ACRIMA.json\n",
      "Loaded 340 items from /data/datasets/OmniMedVQA/OmniMedVQA/QA_information/Restricted-access/AIDA.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_path = \"/data/datasets/OmniMedVQA/OmniMedVQA/QA_information/Open-access/ACRIMA.json\"\n",
    "with open(json_path, \"r\") as f:\n",
    "    acrima_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(acrima_data)} items from {json_path}\")\n",
    "\n",
    "restricted_json_path = \"/data/datasets/OmniMedVQA/OmniMedVQA/QA_information/Restricted-access/AIDA.json\"\n",
    "with open(restricted_json_path, \"r\") as f:\n",
    "    aida_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(aida_data)} items from {restricted_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a17ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'ACRIMA',\n",
       " 'question_id': 'ACRIMA_0000',\n",
       " 'question_type': 'Modality Recognition',\n",
       " 'question': 'What imaging technique was employed to obtain this picture?',\n",
       " 'gt_answer': 'Fundus imaging',\n",
       " 'image_path': 'Images/ACRIMA/Im553_g_ACRIMA.png',\n",
       " 'option_A': 'PET scan',\n",
       " 'option_B': 'CT scan',\n",
       " 'option_C': 'Blood test',\n",
       " 'option_D': 'Fundus imaging',\n",
       " 'modality_type': 'Fundus Photography'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example data from the open-access dataset\n",
    "acrima_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b679c42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'AIDA',\n",
       " 'question_id': 'AIDA_0000',\n",
       " 'question_type': 'Modality Recognition',\n",
       " 'question': 'What imaging modality was used to capture this image?',\n",
       " 'gt_answer': 'Confocal laser endomicroscopy',\n",
       " 'image_path': '${dataset_root_path}/AIDA-E_1/CLE_celiachy_test/test_036_VA_26.jpg',\n",
       " 'option_A': 'Angiography',\n",
       " 'option_B': 'Confocal laser endomicroscopy',\n",
       " 'option_C': 'Nuclear medicine imaging',\n",
       " 'option_D': 'Thermography',\n",
       " 'modality_type': 'Endoscopy'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example data from the restricted-access dataset\n",
    "aida_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52faee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Open-access directory: ['PAD-UFES-20.json', 'Monkeypox Skin Image 2022.json', 'CoronaHack.json', 'Covid CT.json', 'ISIC2019.json', 'NLM- Malaria Data.json', 'OCT & X-Ray 2017.json', 'DeepDRiD.json', 'PALM2019.json', 'Pulmonary Chest Shenzhen.json', 'MHSMA.json', 'Mura.json', 'SARS-CoV-2 CT-scan.json', 'Covid-19 tianchi.json', 'Knee Osteoarthritis.json', 'ISIC2020.json', 'COVIDx CXR-4.json', 'OLIVES.json', 'Fitzpatrick 17k.json', 'Adam Challenge.json', 'HuSHeM.json', 'Chest CT Scan.json', 'BioMediTech.json', 'DRIMDB.json', 'Pulmonary Chest MC.json', 'Retinal OCT-C8.json', 'ALL Challenge.json', 'Blood Cell.json', 'RUS CHN.json', 'MIAS.json', 'ACRIMA.json', 'ISIC2018.json', 'ISBI2016.json', 'BreakHis.json', 'Covid19 heywhale.json', 'JSIEC.json', 'Chest X-Ray PA.json', 'Yangxi.json', 'RadImageNet.json', 'MAlig Lymph.json', 'Diabetic Retinopathy.json', 'CRC100k.json']\n",
      "Files in Restricted-access directory: ['His Can Det.json', 'AIDA.json', 'GAMMA.json', 'Br35h.json', 'Cervix93.json', 'Glaucoma Detection.json', 'CornealNerve.json', 'AVN Assessment.json', 'Oral Cancer kaggle.json', 'MRL Eye.json', 'MED-NODE.json', 'lc25000.json', 'DigestPath19.json', 'PH2.json', 'MINIJSRT.json', 'Kvasir.json', 'Dental Condition Dataset.json', 'BACH2018.json', 'Messidor-2.json', 'TCB Challenge.json', 'BCNB.json', 'Cervical Cancer Screening.json', 'Nerve Tortuosity.json', 'AIROGS.json', 'SIIM-ACR.json', 'APTOS2019 Blindness.json', 'BRIGHT Challenge.json', 'COVIDGR.json', 'Cataract dataset kaggle.json', 'Refuge2.json', '3D Modality.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "open_access_dir = \"/data/datasets/OmniMedVQA/OmniMedVQA/QA_information/Open-access\"\n",
    "open_access_files = os.listdir(open_access_dir)\n",
    "print(\"Files in Open-access directory:\", open_access_files)\n",
    "\n",
    "restricted_access_dir = \"/data/datasets/OmniMedVQA/OmniMedVQA/QA_information/Restricted-access\"\n",
    "restricted_access_files = os.listdir(restricted_access_dir)\n",
    "print(\"Files in Restricted-access directory:\", restricted_access_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c375d907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in Open-access dataset: 88996\n"
     ]
    }
   ],
   "source": [
    "# Combine all json files in the open-access directory\n",
    "import json\n",
    "\n",
    "open_access_data = []\n",
    "for file in open_access_files:\n",
    "    with open(os.path.join(open_access_dir, file), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        open_access_data.extend(data)\n",
    "\n",
    "print(f\"Total items in Open-access dataset: {len(open_access_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f400be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_data_to_sft_data(data):\n",
    "    \"\"\"\n",
    "    Convert raw data to SFT data\n",
    "\n",
    "    data example:\n",
    "    {\n",
    "        'dataset': 'ACRIMA',\n",
    "        'question_id': 'ACRIMA_0000',\n",
    "        'question_type': 'Modality Recognition',\n",
    "        'question': 'What imaging technique was employed to obtain this picture?',\n",
    "        'gt_answer': 'Fundus imaging',\n",
    "        'image_path': 'Images/ACRIMA/Im553_g_ACRIMA.png',\n",
    "        'option_A': 'PET scan',\n",
    "        'option_B': 'CT scan',\n",
    "        'option_C': 'Blood test',\n",
    "        'option_D': 'Fundus imaging',\n",
    "        'modality_type': 'Fundus Photography'\n",
    "    }\n",
    "\n",
    "    SFT data example:\n",
    "    {\n",
    "        \"image\": \"path/to/image.jpg\",\n",
    "        \"problem\": \"question\",\n",
    "        \"solution\": \"answer\"\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        data: list of raw data\n",
    "    Returns:\n",
    "        list of SFT data\n",
    "    \"\"\"\n",
    "    sft_data = []\n",
    "    for item in data:\n",
    "        required_keys = [\"image_path\", \"question\", \"gt_answer\"]\n",
    "        # Find all keys that start with \"option_\" in this item and add them to required_keys (if not already present)\n",
    "        option_keys = [k for k in item.keys() if k.startswith(\"option_\")]\n",
    "        assert len(option_keys) > 0, f\"No option keys found in item: {item}\"\n",
    "        required_keys.extend([k for k in option_keys if k not in required_keys])\n",
    "        sft_data.append({\n",
    "            \"image\": item[\"image_path\"],\n",
    "            \"problem\": item[\"question\"] + \"\\n\" + \"\\n\".join([f\"{k[-1]}: {item[k]}\" for k in option_keys]),\n",
    "            \"solution\": \"<answer> \" + item[\"gt_answer\"] + \" </answer>\"})\n",
    "\n",
    "    return sft_data\n",
    "\n",
    "# Convert open-access data to SFT data\n",
    "open_access_sft_data = convert_raw_data_to_sft_data(open_access_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6f32ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'Images/PAD-UFES-20/uncompresses_data/imgs_part_3/PAT_1449_1554_932.png',\n",
       " 'problem': 'What imaging modality was used to capture the image?\\nA: Positron emission tomography (PET).\\nB: Dermoscopy.\\nC: X-ray.\\nD: CT scan.',\n",
       " 'solution': '<answer> Dermoscopy. </answer>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first item of the SFT data\n",
    "open_access_sft_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd77157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SFT data to a json file\n",
    "with open(\"/data/datasets/OmniMedVQA/OmniMedVQA/open_access_sft_data.json\", \"w\") as f:\n",
    "    json.dump(open_access_sft_data, f)\n",
    "\n",
    "# Load the SFT data from a json file\n",
    "with open(\"/data/datasets/OmniMedVQA/OmniMedVQA/open_access_sft_data.json\", \"r\") as f:\n",
    "    open_access_sft_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c5dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88996/88996 [00:04<00:00, 18297.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import DatasetDict, Dataset, Features, Value, Image as DatasetImage\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "save_path = \"open_access_sft_data_hf\"  # relative path; will be created in current working directory\n",
    "\n",
    "# Define dataset features, storing the image path, not a PIL Image\n",
    "features = Features({\n",
    "    \"image\": DatasetImage(),\n",
    "    \"problem\": Value(\"string\"),\n",
    "    \"solution\": Value(\"string\"),\n",
    "})\n",
    "\n",
    "@contextmanager\n",
    "def pushd(path):\n",
    "    prev = os.getcwd()\n",
    "    os.chdir(path)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.chdir(prev)\n",
    "\n",
    "dataset_root = \"/data/datasets/OmniMedVQA/OmniMedVQA\"\n",
    "\n",
    "hf_dict = {\n",
    "    \"image\": [],\n",
    "    \"problem\": [],\n",
    "    \"solution\": [],\n",
    "}\n",
    "\n",
    "def load_image_from_path(image_path):\n",
    "    try:\n",
    "        img = PILImage.open(image_path)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {str(e)}. Image path: {image_path}\")\n",
    "        return None\n",
    "\n",
    "with pushd(dataset_root):\n",
    "    for item in tqdm(open_access_sft_data):\n",
    "        hf_dict[\"image\"].append(load_image_from_path(item[\"image\"]))\n",
    "        hf_dict[\"problem\"].append(item[\"problem\"])\n",
    "        hf_dict[\"solution\"].append(item[\"solution\"])\n",
    "\n",
    "    # Place the SFT data into the 'train' split of a DatasetDict\n",
    "    train_dataset = Dataset.from_dict(hf_dict, features=features)\n",
    "    open_access_sft_dataset = DatasetDict({\"train\": train_dataset})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bb425f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1620x1620 at 0x798D0E3D29D0>, 'problem': 'What imaging modality was used to capture the image?\\nA: Positron emission tomography (PET).\\nB: Dermoscopy.\\nC: X-ray.\\nD: CT scan.', 'solution': '<answer> Dermoscopy. </answer>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (8/138 shards):   6%|▌         | 5160/88996 [00:25<06:55, 201.78 examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Mask must be a pyarrow.Array of type boolean",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_dataset[\u001b[32m0\u001b[39m])\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save the dataset to disk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mopen_access_sft_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/dataset_dict.py:1360\u001b[39m, in \u001b[36mDatasetDict.save_to_disk\u001b[39m\u001b[34m(self, dataset_dict_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[39m\n\u001b[32m   1358\u001b[39m     json.dump({\u001b[33m\"\u001b[39m\u001b[33msplits\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)}, f)\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items():\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposixpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/arrow_dataset.py:1641\u001b[39m, in \u001b[36mDataset.save_to_disk\u001b[39m\u001b[34m(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[39m\n\u001b[32m   1639\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m   1640\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m1641\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_save_to_disk_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1642\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1643\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/arrow_dataset.py:1674\u001b[39m, in \u001b[36mDataset._save_to_disk_single\u001b[39m\u001b[34m(job_id, shard, fpath, storage_options)\u001b[39m\n\u001b[32m   1672\u001b[39m _time = time.time()\n\u001b[32m   1673\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pa_table \u001b[38;5;129;01min\u001b[39;00m shard.with_format(\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m).iter(batch_size):\n\u001b[32m-> \u001b[39m\u001b[32m1674\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1675\u001b[39m     num_examples_progress_update += \u001b[38;5;28mlen\u001b[39m(pa_table)\n\u001b[32m   1676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m time.time() > _time + config.PBAR_REFRESH_TIME_INTERVAL:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/arrow_writer.py:716\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    714\u001b[39m pa_table = table_cast(pa_table, \u001b[38;5;28mself\u001b[39m._schema)\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed_local_files:\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     pa_table = \u001b[43membed_table_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[38;5;28mself\u001b[39m._num_bytes += pa_table.nbytes\n\u001b[32m    718\u001b[39m \u001b[38;5;28mself\u001b[39m._num_examples += pa_table.num_rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/table.py:2248\u001b[39m, in \u001b[36membed_table_storage\u001b[39m\u001b[34m(table, token_per_repo_id)\u001b[39m\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Features, require_storage_embed\n\u001b[32m   2247\u001b[39m features = Features.from_arrow_schema(table.schema)\n\u001b[32m-> \u001b[39m\u001b[32m2248\u001b[39m arrays = \u001b[43m[\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_array_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequire_storage_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2251\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2253\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_arrays(arrays, schema=features.arrow_schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/table.py:2249\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Features, require_storage_embed\n\u001b[32m   2247\u001b[39m features = Features.from_arrow_schema(table.schema)\n\u001b[32m   2248\u001b[39m arrays = [\n\u001b[32m-> \u001b[39m\u001b[32m2249\u001b[39m     \u001b[43membed_array_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m require_storage_embed(feature)\n\u001b[32m   2251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m table[name]\n\u001b[32m   2252\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m   2253\u001b[39m ]\n\u001b[32m   2254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_arrays(arrays, schema=features.arrow_schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/table.py:1795\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(array, *args, **kwargs):\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa.ChunkedArray):\n\u001b[32m-> \u001b[39m\u001b[32m1795\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array(\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1797\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(array, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/table.py:1795\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1793\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(array, *args, **kwargs):\n\u001b[32m   1794\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa.ChunkedArray):\n\u001b[32m-> \u001b[39m\u001b[32m1795\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1797\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(array, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/table.py:2124\u001b[39m, in \u001b[36membed_array_storage\u001b[39m\u001b[34m(array, feature, token_per_repo_id)\u001b[39m\n\u001b[32m   2122\u001b[39m     array = array.storage\n\u001b[32m   2123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[33m\"\u001b[39m\u001b[33membed_storage\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(array.type):\n\u001b[32m   2126\u001b[39m     \u001b[38;5;66;03m# feature must be a dict\u001b[39;00m\n\u001b[32m   2127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/datasets/features/image.py:306\u001b[39m, in \u001b[36mImage.embed_storage\u001b[39m\u001b[34m(self, storage, token_per_repo_id)\u001b[39m\n\u001b[32m    295\u001b[39m bytes_array = pa.array(\n\u001b[32m    296\u001b[39m     [\n\u001b[32m    297\u001b[39m         (path_to_bytes(x[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m x[\u001b[33m\"\u001b[39m\u001b[33mbytes\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     \u001b[38;5;28mtype\u001b[39m=pa.binary(),\n\u001b[32m    301\u001b[39m )\n\u001b[32m    302\u001b[39m path_array = pa.array(\n\u001b[32m    303\u001b[39m     [os.path.basename(path) \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m storage.field(\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m).to_pylist()],\n\u001b[32m    304\u001b[39m     \u001b[38;5;28mtype\u001b[39m=pa.string(),\n\u001b[32m    305\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m storage = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStructArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbytes_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_array\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbytes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbytes_array\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_null\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m array_cast(storage, \u001b[38;5;28mself\u001b[39m.pa_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/pyarrow/array.pxi:4271\u001b[39m, in \u001b[36mpyarrow.lib.StructArray.from_arrays\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/med-r1/lib/python3.11/site-packages/pyarrow/array.pxi:4941\u001b[39m, in \u001b[36mpyarrow.lib.c_mask_inverted_from_obj\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Mask must be a pyarrow.Array of type boolean"
     ]
    }
   ],
   "source": [
    "with pushd(dataset_root):\n",
    "    print(train_dataset[0])\n",
    "\n",
    "    # Save the dataset to disk\n",
    "    open_access_sft_dataset.save_to_disk(os.path.join(save_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "678da9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1444x1444 at 0x798D12C0B950>, 'problem': 'Is there any indication of abnormalities in these images?\\nA: In this image, there are no apparent abnormalities. It represents a normal or fundus of high myopia.\\nB: This image shows a severe abnormality in the fundus.\\nC: The abnormalities in this image are consistent with age-related macular degeneration.\\nD: The anomalies in this image indicate the presence of glaucoma.', 'solution': '<answer> In this image, there are no apparent abnormalities. It represents a normal or fundus of high myopia. </answer>'}\n"
     ]
    }
   ],
   "source": [
    "with pushd(dataset_root):\n",
    "    print(train_dataset[5162])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279a5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'problem', 'solution'],\n",
       "        num_rows: 88996\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
